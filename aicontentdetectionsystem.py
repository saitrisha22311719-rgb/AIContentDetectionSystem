# -*- coding: utf-8 -*-
"""Copy of Yet another copy of MBert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T0Ynf9ueoeTWPBdf7e_dQcUnWceAmGOB
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Adjust path based on where you uploaded in Drive
csv_path = "/content/drive/MyDrive/Training_Essay_Data.csv"
df = pd.read_csv(csv_path)

print("‚úÖ Data Loaded! Shape:", df.shape)
df.head()

!pip install transformers datasets scikit-learn

import pandas as pd

df = pd.read_csv(csv_path) #loads csv file located at csv_path
print("Dataset shape:", df.shape) #prints shape of dataset i.e no of rows and columns
print(df.head()) #shows first 5 rows of dataframe

import pandas as pd

# Load dataset from Drive
csv_path = "/content/drive/MyDrive/Training_Essay_Data.csv"
df = pd.read_csv(csv_path) #Reads the CSV file into a pandas DataFrame called df

print("‚úÖ Data Loaded. Shape:", df.shape) #Prints a confirmation message that the dataset is loaded. tells how big the dataset is
print(df.head())

# drop_duplicates() removes duplicate rows from the DataFrame.
df = df.drop_duplicates()
df = df.dropna(subset=["text", "generated"])

print("After cleaning:", df.shape)

# Check label distribution
print(df['generated'].value_counts())

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
df['label_encoded'] = encoder.fit_transform(df['generated'])

print("Classes:", encoder.classes_)
df.head()

from sklearn.model_selection import train_test_split

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'], df['label_encoded'],
    test_size=0.1,
    stratify=df['label_encoded'],
    random_state=42
)

print("Train size:", len(train_texts))
print("Validation size:", len(val_texts))

!pip install transformers datasets -q

from transformers import BertTokenizer, BertForSequenceClassification

MODEL_NAME = "bert-base-multilingual-cased"  # mBERT

tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)

import torch
from datasets import Dataset

train_dataset = Dataset.from_dict({"text": train_texts, "labels": train_labels})
val_dataset = Dataset.from_dict({"text": val_texts, "labels": val_labels})

def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset = val_dataset.map(tokenize, batched=True)

train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

2

# Check your columns
print(df.columns)

# Rename columns to standard names
# Replace 'generated' with your actual column name for labels
# Replace 'text' with your actual column name for text content
df = df.rename(columns={'generated': 'label', 'text': 'title'})

# Optional: Convert boolean to string labels if needed
df['label'] = df['label'].apply(lambda x: "AI-generated" if x else "Not AI-generated")

# Remove any nulls
df = df.dropna(subset=['label', 'title'])

# Confirm
print(df.head())
print(df.columns)

!pip install transformers sentencepiece

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

model_name = "tuner007/pegasus_paraphrase"
para_tokenizer = AutoTokenizer.from_pretrained(model_name)
para_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

def paraphrase_pegasus(text):
    batch = para_tokenizer([text], truncation=True, padding="longest", return_tensors="pt").to(para_model.device)
    translated = para_model.generate(**batch, max_length=60, num_beams=5, num_return_sequences=1)
    return para_tokenizer.decode(translated[0], skip_special_tokens=True)

print(paraphrase_pegasus(df['title'].iloc[0]))

!pip install tqdm
from tqdm.notebook import tqdm
tqdm.pandas()



from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import AutoTokenizer

# 1Ô∏è‚É£ Map string labels to integers
label2id = {"Not AI-generated": 0, "AI-generated": 1}
df['label'] = df['label'].map(label2id)

# 2Ô∏è‚É£ Remove any NaN rows
df = df.dropna(subset=['title', 'label'])

# 3Ô∏è‚É£ Split into train and validation
train_df, val_df = train_test_split(
    df, test_size=0.1, random_state=42, stratify=df['label']
)

# 4Ô∏è‚É£ Convert to HuggingFace Dataset
train_dataset = Dataset.from_pandas(train_df)
val_dataset   = Dataset.from_pandas(val_df)

# 5Ô∏è‚É£ Initialize mBERT tokenizer
MODEL_NAME = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# 6Ô∏è‚É£ Tokenization function
def tokenize_fn(batch):
    return tokenizer(batch["title"], padding="max_length", truncation=True, max_length=256)

# 7Ô∏è‚É£ Tokenize datasets
tokenized_train = train_dataset.map(tokenize_fn, batched=True)
tokenized_val   = val_dataset.map(tokenize_fn, batched=True)

# 8Ô∏è‚É£ Set format for PyTorch
tokenized_train.set_format("torch", columns=["input_ids", "attention_mask", "label"])
tokenized_val.set_format("torch", columns=["input_ids", "attention_mask", "label"])

print("‚úÖ Tokenization & dataset prep complete!")
print(f"Train samples: {len(tokenized_train)}, Validation samples: {len(tokenized_val)}")

from transformers import AutoModelForSequenceClassification

# Initialize mBERT for sequence classification
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-multilingual-cased",
    num_labels=2  # AI-generated vs Not AI-generated
)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",            # directory to save checkpoints
    num_train_epochs=3,                # total epochs
    per_device_train_batch_size=16,    # batch size for training
    per_device_eval_batch_size=16,     # batch size for evaluation
    learning_rate=2e-5,                # learning rate
    weight_decay=0.01,                 # regularization
    logging_dir='./logs',              # logs directory
    logging_steps=50                   # logging frequency
)

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset

# Load mBERT tokenizer & model
tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")
model = BertForSequenceClassification.from_pretrained("bert-base-multilingual-cased", num_labels=2)

# Tokenize function
def tokenize_function(examples):
    return tokenizer(
        examples["title"],  # column containing text
        truncation=True,
        padding="max_length",
        max_length=128
    )

# Map tokenization
tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_val = val_dataset.map(tokenize_function, batched=True)

# Set format for PyTorch
tokenized_train.set_format("torch", columns=["input_ids", "attention_mask", "label"])
tokenized_val.set_format("torch", columns=["input_ids", "attention_mask", "label"])

# Training arguments (compatible with older transformers)
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=3e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
    report_to="none"  # disables wandb
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer
)

# Train
trainer.train()





from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np

# Define compute_metrics function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)  # get predicted class indices

    # Calculate precision, recall, F1-score for each class and average (weighted)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted'
    )
    accuracy = accuracy_score(labels, predictions)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# Example usage with Hugging Face Trainer
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=50,
    report_to="none"  # disable wandb to avoid login issues
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics   # <-- attach metrics function
)

# Evaluate the model
metrics = trainer.evaluate()
print("Evaluation metrics:")
print(metrics)

# Save locally (corrected)
save_path = "my_finetuned_mbert"  # no './' at start
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

from transformers import BertForSequenceClassification, BertTokenizer

model = BertForSequenceClassification.from_pretrained("my_finetuned_mbert")
tokenizer = BertTokenizer.from_pretrained("my_finetuned_mbert")

from sklearn.metrics import roc_auc_score
import torch
import numpy as np

# Get raw predictions (probabilities)
predictions = trainer.predict(tokenized_val) # Use the tokenized validation dataset
probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=-1)[:,1]  # AI class prob
y_true = predictions.label_ids

auc = roc_auc_score(y_true, probs.numpy())
print("ROC-AUC:", auc)

import sys, subprocess
import torch

# ensure sklearn is installed
try:
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
except Exception:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "scikit-learn"])
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix

from transformers import Trainer, DataCollatorWithPadding, TrainingArguments

# pick validation dataset
if 'tokenized_val' in globals():
    val_ds = tokenized_val
elif 'val_dataset' in globals():
    val_ds = val_dataset
else:
    raise RuntimeError("No validation dataset found (need tokenized_val or val_dataset).")

# Ensure labels column is correct
if "label" in val_ds.column_names and "labels" not in val_ds.column_names:
    val_ds = val_ds.rename_column("label", "labels")

# Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# dummy training args (Trainer always needs them)
training_args = TrainingArguments(
    output_dir="./results",
    per_device_eval_batch_size=16,
    report_to="none",   # prevents WandB / HF Hub logging
    logging_dir="./logs",
)

# create trainer for prediction
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

# Run prediction on validation dataset
pred_out = trainer.predict(val_ds)

# Compute metrics
def compute_metrics_from_preds(predictions, label_ids):
    if predictions.ndim > 1:
        preds = predictions.argmax(-1)
    else:
        preds = predictions
    labels = label_ids
    avg = "binary" if getattr(model.config, "num_labels", 2) == 2 else "macro"
    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=avg, zero_division=0)
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1, "preds": preds, "labels": labels}

result = compute_metrics_from_preds(pred_out.predictions, pred_out.label_ids)

print("\n=== Evaluation Metrics ===")
print(f"Accuracy:  {result['accuracy']:.4f}")
print(f"Precision: {result['precision']:.4f}")
print(f"Recall:    {result['recall']:.4f}")
print(f"F1 Score:  {result['f1']:.4f}")

print("\nClassification report:")
print(classification_report(result['labels'], result['preds'], digits=4))

print("Confusion matrix:")
print(confusion_matrix(result['labels'], result['preds']))

# -*- coding: utf-8 -*-
import sys, subprocess
import torch

# ensure sklearn is installed
try:
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
except Exception:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "scikit-learn"])
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix

from transformers import Trainer, DataCollatorWithPadding, TrainingArguments
from transformers import BertTokenizer, BertForSequenceClassification

# Load the saved model and tokenizer
model = BertForSequenceClassification.from_pretrained("my_finetuned_mbert")
tokenizer = BertTokenizer.from_pretrained("my_finetuned_mbert")


texts = [
    'Most Pegasus models are trained on English text, so they don‚Äôt handle Hindi very well for paraphrasing or summarization'
]


# Tokenize
inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="pt")

# Move inputs to the same device as the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
inputs = {name: tensor.to(device) for name, tensor in inputs.items()}
model.to(device) # Ensure model is on the correct device

# Predict
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=-1)

# Map predictions to labels
labels_map = {0: "Not AI-generated", 1: "AI-generated"}
predicted_labels = [labels_map[p.item()] for p in predictions]

print("Predictions:", predicted_labels)

# Rephrase AI-generated predictions using Pegasus
predicted_titles = [
    'Most Pegasus models are trained on English text, so they don‚Äôt handle Hindi very well for paraphrasing or summarization'

]  # replace with your actual texts if predicting multiple

# Only rephrase if predicted as AI-generated
rephrased_titles = []
for text, label in zip(predicted_titles, predicted_labels):
    if label == "AI-generated":
        rephrased_text = paraphrase_pegasus(text)
    else:
        rephrased_text = text
    rephrased_titles.append(rephrased_text)

print("Rephrased Titles:", rephrased_titles)

# -*- coding: utf-8 -*-
import sys, subprocess
import torch

# ensure sklearn is installed
try:
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
except Exception:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "scikit-learn"])
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix

from transformers import Trainer, DataCollatorWithPadding, TrainingArguments
from transformers import BertTokenizer, BertForSequenceClassification

# Load the saved model and tokenizer
model = BertForSequenceClassification.from_pretrained("my_finetuned_mbert")
tokenizer = BertTokenizer.from_pretrained("my_finetuned_mbert")


texts = [
   'English text is the main language used by most Pegasus models, so they dont handle Hindi very well.'

]


# Tokenize
inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="pt")

# Move inputs to the same device as the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
inputs = {name: tensor.to(device) for name, tensor in inputs.items()}
model.to(device) # Ensure model is on the correct device

# Predict
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=-1)

# Map predictions to labels
labels_map = {0: "Not AI-generated", 1: "AI-generated"}
predicted_labels = [labels_map[p.item()] for p in predictions]

print("Predictions:", predicted_labels)

import torch
from langdetect import detect, DetectorFactory

# For consistent language detection
DetectorFactory.seed = 0

# --- Load your trained multi-class mBERT model and tokenizer ---
# model = AutoModelForSequenceClassification.from_pretrained("my_finetuned_mbert_multiclass")
# tokenizer = AutoTokenizer.from_pretrained("my_finetuned_mbert_multiclass")

def predict_mbert_confidence(text, min_conf=0.6):
    """
    Predicts which model (or Human) generated the text.
    Returns predicted label, confidence score, authenticity level, and detected language.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    # Detect language
    try:
        lang = detect(text)
    except:
        lang = "unknown"

    # Tokenize input
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1).squeeze()
        pred_id = torch.argmax(probs).item()
        confidence = float(probs[pred_id])

    # Multi-class labels including Gemini
    id2label = {
        0: "Human",
        1: "GPT-3",
        2: "ChatGPT",
        3: "Gemini",
        4: "Other AI"
    }
    predicted_model = id2label[pred_id]

    # Confidence-based authenticity
    if confidence < min_conf:
        authenticity = "Indeterminate"
    else:
        if predicted_model == "Human":
            authenticity = "Very likely Human" if confidence > 0.8 else "Mixed Human"
        else:
            authenticity = f"Very likely {predicted_model}" if confidence > 0.8 else "Mixed AI"

    return {
        "prediction": predicted_model,
        "confidence": confidence,
        "authenticity_level": authenticity,
        "detected_language": lang,
        "all_model_confidences": {id2label[i]: float(probs[i]) for i in range(len(probs))}
    }

# --- Example usage ---
sample_texts = [
    "‡§Ü‡§ú ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§Ø‡•ã‡§π‡§æ‡§∞‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡•å‡§∏‡§Æ ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§",
    "This article discusses the potential of large language models in solving global issues.",
    "GPT-3 can generate human-like text in multiple languages.",
    "Gemini AI is shaping the future of conversational agents.",
    "‡§Ü‡§ú ‡§Ü‡§ï‡§æ‡§∂ ‡§ñ‡•Ç‡§™ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§Ü‡§π‡•á ‡§Ü‡§£‡§ø ‡§∏‡•Ç‡§∞‡•ç‡§Ø‡§™‡•ç‡§∞‡§ï‡§æ‡§∂ ‡§ö‡§Æ‡§ï‡§§ ‡§Ü‡§π‡•á",
    "Ïò§Îäò ÎÇ†Ïî®Í∞Ä Îß§Ïö∞ ÎßëÍ≥† ÌñáÎ≥ïÏù¥ Îî∞ÎúªÌïòÎã§",
    "Hoy el clima es soleado y c√°lido."
]

for txt in sample_texts:
    result = predict_mbert_confidence(txt)
    print(f"\nText: {txt}\nPrediction: {result}")

import torch
from langdetect import detect
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# ------------------------------
# Your trained mBERT classifier
# ------------------------------
# model = AutoModelForSequenceClassification.from_pretrained("my_finetuned_mbert")
# tokenizer = AutoTokenizer.from_pretrained("my_finetuned_mbert")

# ------------------------------
# mT5 paraphrasing model
# ------------------------------
para_model_name = "csebuetnlp/mT5_multilingual_XLSum"
para_tokenizer = AutoTokenizer.from_pretrained(para_model_name)
para_model = AutoModelForSeq2SeqLM.from_pretrained(para_model_name).to(
    "cuda" if torch.cuda.is_available() else "cpu"
)

# ------------------------------
# mBERT confidence-based prediction
# ------------------------------
def predict_mbert_confidence(text, min_conf=0.6):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    inputs = tokenizer(
        text, return_tensors="pt", truncation=True, padding=True, max_length=256
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1).squeeze()
        pred_id = torch.argmax(probs).item()
        confidence = float(probs[pred_id])

    id2label = {0: "human", 1: "ai"}
    predicted_model = id2label[pred_id]

    if confidence < min_conf:
        authenticity = "Indeterminate"
    else:
        if predicted_model == "human":
            authenticity = "Very likely Human" if confidence > 0.8 else "Mixed"
        else:
            authenticity = "Very likely AI" if confidence > 0.8 else "Mixed"

    return {
        "prediction": "Human" if predicted_model == "human" else "AI Generated",
        "confidence": confidence,
        "authenticity_level": authenticity,
        "all_model_confidences": {id2label[i]: float(probs[i]) for i in range(len(probs))}
    }

# ------------------------------
# Paraphrase AI-generated text using mT5 (same language preservation)
# ------------------------------
def paraphrase_multilingual(text, max_length=128):
    try:
        lang = detect(text)
    except:
        lang = "unknown"

    # Prefix encourages human-like phrasing
    prefix = "paraphrase in a natural human style: "
    input_text = f"{prefix}{text} </s>"

    inputs = para_tokenizer(
        [input_text], truncation=True, padding="longest", return_tensors="pt"
    ).to(para_model.device)

    with torch.no_grad():
        outs = para_model.generate(
            **inputs,
            max_length=max_length,
            num_beams=8,
            do_sample=True,
            temperature=1.0,
            top_p=0.9,
            early_stopping=True
        )

    rephrased = para_tokenizer.decode(outs[0], skip_special_tokens=True)

    # Optional: enforce same-language output
    if lang != "unknown":
        # Simple check: if detected language is Hindi and output is mostly English, re-translate back
        if lang == "hi" and all(ord(c) < 128 for c in rephrased):
            rephrased = f"(Hindi) {rephrased}"
        elif lang == "en" and any(ord(c) > 128 for c in rephrased):
            rephrased = f"(English) {rephrased}"

    return rephrased

# ------------------------------
# Combined function: predict + paraphrase
# ------------------------------
def analyze_and_paraphrase(text):
    print("\n---")
    print("Original:", text)

    res = predict_mbert_confidence(text)
    print(
        f"Prediction: {res['prediction']} | Confidence: {res['confidence']:.2f} | {res['authenticity_level']}"
    )

    if res['prediction'] == "AI Generated":
        rephrased = paraphrase_multilingual(text)
        print("Paraphrased to Human-like:", rephrased)
    else:
        print("No paraphrasing needed (predicted as Human).")

# ------------------------------
# Example usage
# ------------------------------
sample_texts = [
    "This article discusses the potential of large language models in solving global issues.",
    "‡§Ü‡§ú ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§Ø‡•ã‡§π‡§æ‡§∞‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡•å‡§∏‡§Æ ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§",
    "Machine learning is changing the way we approach problem-solving.",
    "‡§Æ‡•à‡§Ç‡§®‡•á ‡§Ü‡§ú ‡§¨‡§π‡•Å‡§§ ‡§∏‡•Å‡§®‡•ç‡§¶‡§∞ ‡§ï‡§µ‡§ø‡§§‡§æ ‡§≤‡§ø‡§ñ‡•Ä‡•§"
]

for txt in sample_texts:
    analyze_and_paraphrase(txt)

# -*- coding: utf-8 -*-
import sys, subprocess
import torch

# ensure sklearn is installed
try:
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
except Exception:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "scikit-learn"])
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix

from transformers import Trainer, DataCollatorWithPadding, TrainingArguments
from transformers import BertTokenizer, BertForSequenceClassification

# Load the saved model and tokenizer
model = BertForSequenceClassification.from_pretrained("my_finetuned_mbert")
tokenizer = BertTokenizer.from_pretrained("my_finetuned_mbert")


texts = [
   '‡§Ø‡§π ‡§ï‡§µ‡§ø‡§§‡§æ ‡§Ü‡§™‡§®‡•á ‡§ï‡§≠‡•Ä ‡§∏‡•Å‡§®‡•Ä ‡§π‡•ã‡§ó‡•Ä?'

]


# Tokenize
inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="pt")

# Move inputs to the same device as the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
inputs = {name: tensor.to(device) for name, tensor in inputs.items()}
model.to(device) # Ensure model is on the correct device

# Predict
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=-1)

# Map predictions to labels
labels_map = {0: "Not AI-generated", 1: "AI-generated"}
predicted_labels = [labels_map[p.item()] for p in predictions]

print("Predictions:", predicted_labels)

!pip install langdetect -q













sample_texts = [
    "‡§Ü‡§ú ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§Ø‡•ã‡§π‡§æ‡§∞‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡•å‡§∏‡§Æ ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§",
    "This article discusses the potential of large language models in solving global issues.",
    "‡§Æ‡•à‡§Ç‡§®‡•á ‡§Ü‡§ú ‡§¨‡§π‡•Å‡§§ ‡§∏‡•Å‡§®‡•ç‡§¶‡§∞ ‡§ï‡§µ‡§ø‡§§‡§æ ‡§≤‡§ø‡§ñ‡•Ä‡•§",
    "I am isha"
]

for txt in sample_texts:
    print(f"\nText: {txt}\nPrediction: {predict_human_ai(txt)}")

sample_texts = [
    "‡§Æ‡•Ä ‡§à‡§∂‡§æ ‡§Ü‡§π‡•á",
    "A llama is a domesticated South American member of the camel family, known for its long neck, thick woolly coat, and use as a pack animal",
    "I am Ishaa"
]

for txt in sample_texts:
    print(f"\nText: {txt}\nPrediction: {predict_combined(txt)}")

def analyze_and_paraphrase(text):
    print("\nüìù Original Text:", text)
    result = predict_human_ai(text)
    print(f"ü§ñ Prediction: {result['prediction']} | Confidence: {result['confidence']:.2f}")
    print(f"üß† Authenticity: {result['authenticity_level']}")

    # Only paraphrase if predicted as AI-generated
    if result['prediction'] == "AI Generated":
        rephrased = paraphrase_multilingual(text)
        print(f"‚ú® Paraphrased Version: {rephrased}\n")
    else:
        print("‚ú® No paraphrasing needed (predicted as Human)\n")



sample_texts = [
    "‡§Ü‡§ú ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§Ø‡•ã‡§π‡§æ‡§∞‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡•å‡§∏‡§Æ ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§",
    "The future of artificial intelligence is full of possibilities.",
    "‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡§º‡•â‡§∞‡•ç‡§Æ‡•á‡§∂‡§® ‡§§‡•á‡§ú‡§º‡•Ä ‡§∏‡•á ‡§¨‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•à ‡§î‡§∞ ‡§π‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§™‡§∞ ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ ‡§°‡§æ‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à‡•§",
    "Technology enables faster innovation and smarter solutions."
]

for txt in sample_texts:
    analyze_and_paraphrase(txt)

sample_texts = [
    "‡§Æ‡•Ä ‡§à‡§∂‡§æ ‡§Ü‡§π‡•á"
]

for txt in sample_texts:
    analyze_and_paraphrase(txt)

sample_texts = [
    "That gives a more visual, dashboard-like result"
]

for txt in sample_texts:
    analyze_and_paraphrase(txt)

def predict_human_ai(text, min_conf=0.6):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=256).to(device)
    with torch.no_grad():
        logits = clf_model(**inputs).logits
        probs = torch.nn.functional.softmax(logits, dim=1).squeeze().cpu().numpy()
    p_human, p_ai = float(probs[0]), float(probs[1])
    if p_ai > p_human:
        label = "AI Generated"
        conf = p_ai
    else:
        label = "Human"
        conf = p_human
    if conf < min_conf:
        authenticity = "Indeterminate"
    else:
        authenticity = "Very likely AI" if label=="AI Generated" and conf>0.8 else \
                       "Mixed" if conf<=0.8 and conf>=0.6 else \
                       "Very likely Human"
    return {"prediction": label, "confidence": conf, "authenticity": authenticity}

def analyze_and_paraphrase(text):
    print("\n---")
    print("Original:", text)
    res = predict_human_ai(text)
    print(f"Prediction: {res['prediction']} | Confidence: {res['confidence']:.2f} | {res['authenticity']}")
    try:
        lang = detect(text)
    except:
        lang = "unknown"
    print("Detected language:", lang)
    paraphr = paraphrase_multilingual(text)
    print("Paraphrased:", paraphr)

sample_texts = [
    "‡§Ü‡§ú ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§Ø‡•ã‡§π‡§æ‡§∞‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡•å‡§∏‡§Æ ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§",
    "The future of artificial intelligence is full of possibilities.",
    "‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡§º‡•â‡§∞‡•ç‡§Æ‡•á‡§∂‡§® ‡§§‡•á‡§ú‡§º‡•Ä ‡§∏‡•á ‡§¨‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•à ‡§î‡§∞ ‡§π‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§™‡§∞ ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ ‡§°‡§æ‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à‡•§",
    "Technology enables faster innovation and smarter solutions."
]

for s in sample_texts:
    analyze_and_paraphrase(s)

sample_texts = [
    "‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§ï‡•Ä ‡§∞‡•ã‡§∂‡§®‡•Ä ‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ ‡§ï‡•Ä ‡§ï‡§≤‡§Æ ‡§∏‡•á ‡§≤‡§ø‡§ñ‡•Ä¬†‡§ú‡§æ¬†‡§∞‡§π‡•Ä¬†‡§π‡•à‡•§"
]

for s in sample_texts:
    analyze_and_paraphrase(s)

